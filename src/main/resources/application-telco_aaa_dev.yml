# Server Configuration
quarkus:
  http:
    root-path: /api/v1
    port: 9905
    io-threads: 16                  # Increased for 285 TPS/pod (~18 TPS per thread)
    limits:
      max-body-size: 10M
      max-connections: 1000         # Sufficient for 285 TPS/pod with margin


  datasource:
    db-kind: oracle
    username: aaa
    password: Aaa!89nky78D
    reactive:
      url: oracle:thin:@aaa-dev.cn8eyeqma3tp.ap-southeast-2.rds.amazonaws.com:1521/ORCL
      max-size: 200
      idle-timeout: PT10M
      acquisition-timeout: PT5S

  # Vertx Configuration - Optimized for 2000 TPS across 7 pods (~285 TPS/pod)
  vertx:
    event-loops-pool-size: 32       # ~9 TPS per event loop - good balance
    warning-exception-time: 10s
    max-event-loop-execute-time: 5s
    max-worker-execute-time: 60s
    prefer-native-transport: true
    caching: true


  thread-pool:
    core-threads: 100
    max-threads: 500
    queue-size: 5000
    growth-resistance: 0
    shutdown-timeout: 30s

  console:
    color: true
  log:
    level: INFO
    file:
      path: /var/dte/aaa-account.log
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
      rotation:
        max-file-size: 100M
        max-backup-index: 10
        rotate-on-boot: true
    console:
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
    category:
      "org.apache.kafka":
        level: INFO
      "io.smallrye.reactive.messaging.kafka":
        level: INFO
      "io.quarkus.config":
        level: INFO
      "io.smallrye.config":
        level: INFO
      "com.csg.airtel.aaa4j":
        level: INFO

  otel:
    traces:
      exporter: none

  # Redis Configuration - OPTIMIZED FOR 2000 TPS across 7 pods
  # Per pod load: ~285 TPS (2000 TPS / 7 pods)
  # With 4s fault tolerance timeout, need ~200 connections/pod for safety
  redis:
    hosts: redis://redis-cluster.cluster-redis.svc.cluster.local:6379
    password: AAAtest_Redis_Pass_w0rd!
    database: 0
    # Pool configuration optimized for 285 TPS/pod
    max-pool-size: 200              # ~0.7 connections per TPS (285 TPS * 4s timeout / 1000ms * safety margin)
    max-waiting-handlers: 4000      # 20x pool size for burst handling
    max-pool-waiting: 1000          # Sufficient concurrent request capacity for spikes
    pool-recycle-timeout: 240000    # 4 minutes - more frequent recycling for high load
    reconnect-attempts: 3           # Fail faster under load
    reconnect-interval: 500         # 500ms - faster reconnection
    pool-cleaner-interval: 20000    # Clean pool every 20 seconds
    response-timeout: 3000          # 3s timeout - MUST be < CacheClient @Timeout (4000ms)
    connection-timeout: 2000        # 2s connection timeout - quick failure detection
    timeout: 60000                  # Overall connection timeout

# Pool Configuration
pool:
  max-size: 200                     # Max connections - increased for 2000 TPS
  connection-timeout: 5000          # 5 seconds to acquire connection
  idle-timeout: 600000              # 10 minutes idle before closing
  max-lifetime: 1800000             # 30 minutes max connection lifetime
  acquire-increment: 4              # Connections to acquire at once
  prepared-statement-cache-max-size: 256  # Cache prepared statements
  pipelining-enabled: true          # Enable query pipelining
  pipelining-limit: 256             # Max pipelined queries
  event-loop-size: 32               # Match vertx event-loops-pool-size
  tcp-keep-alive: true              # Detect dead connections
  tcp-no-delay: true                # Reduce latency
  pool-cleaner-enabled: true        # Clean idle connections
  pool-cleaner-interval: 60000      # Clean every minute

idle-session:
  enabled: true
  timeout-minutes: 60       # 1 hour default - sessions older than this will be terminated
  scheduler-interval: 60m    # Run every 60 minutes
  batch-size: 1000          # Process 1000 sessions per batch (increased for scale)

# Kafka Configuration
kafka:
  bootstrap:
   servers: confluent-ctrl-0-svc.rkafkacone.svc.cluster.local:9092

mp:
  messaging:
    outgoing:
      db-write-events:
        connector: smallrye-kafka
        topic: DC-DR
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

      accounting-resp-events:
        connector: smallrye-kafka
        topic: accounting-response
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

      accounting-cdr-events:
        connector: smallrye-kafka
        topic: cdr-event
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

      quota-notification-events:
        connector: smallrye-kafka
        topic: quota-notifications
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

    incoming:
      accounting-events:
        connector: smallrye-kafka
        topic: accounting
        group.id: accounting-consumer-group
        value.deserializer: io.quarkus.kafka.client.serialization.ObjectMapperDeserializer
        key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
        auto.offset.reset: earliest
        enable.auto.commit: false
        max.poll.records: 1000          # Increased for better throughput at 285 TPS/pod
        max.poll.interval.ms: 300000
        session.timeout.ms: 30000
        fetch.min.bytes: 128
        fetch.max.bytes: 52428800
        max.partition.fetch.bytes: 10485760
        receive.buffer.bytes: 131072    # Doubled from 65536 for better network throughput
        failure-strategy: ignore

