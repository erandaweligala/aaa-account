quarkus:
  http:
    http2: true
    root-path: /api/v1
    port: 9905
    io-threads: 1                    # 0.6 core pod → 1 I/O thread sufficient
    limits:
      max-body-size: 10M
      max-connections: 200           # Right-sized for 107 TPS/pod

#  datasource:
#    db-kind: oracle
#    username: aaa
#    password: Aaa!89nky78D
#    reactive:
#      #url: oracle:thin:@localhost:1521/FREEPDB1
#      url: oracle:thin:@localhost:1522/ORCL

  datasource:
    db-kind: oracle
    username: aaa
    password: Aaa!89nky78D
    reactive:
      url: oracle:thin:@aaa-dev.cn8eyeqma3tp.ap-southeast-2.rds.amazonaws.com:1521/ORCL


  # Vertx Configuration - Optimized for 1500 TPS (14 pods x 0.6 core / 1GB, ~107 TPS/pod)
  vertx:
    event-loops-pool-size: 1         # 0.6 core pod → 1 event loop sufficient
    internal-blocking-pool-size: 16  # Match consumer concurrency (12) + 4 headroom for other blocking ops
    warning-exception-time: 10s
    max-event-loop-execute-time: 5s
    max-worker-execute-time: 60s
    prefer-native-transport: true
    caching: false

  thread-pool:
    core-threads: 16                 # 12 consumer + 4 for HTTP/scheduled/other ops
    max-threads: 24                  # Modest burst headroom - avoids context switching on 0.6 core
    queue-size: 500                  # Conservative for 1GB memory, backpressure prevents buildup
    growth-resistance: 0
    shutdown-timeout: 30s

  console:
    color: true
  log:
    level: INFO
    console:
      format: "[%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ}] %-5p [%c][%t][%X{traceId}][%X{userName}][%X{sessionId}] %s%e%n"
      async:
        queue-length: 10000
        overflow: discard # Fixed the "discard file not create" typo

    # File Configuration
    file:
      path: /var/log/dte/accounting-service-${HOSTNAME:unknown}.log
      format: "[%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ}] %-5p [%c][%t][%X{traceId}][%X{userName}][%X{sessionId}] %s%e%n"
      async:
        enable: true
        queue-length: 20000
        overflow: discard
      rotation:
        max-file-size: 100M
        max-backup-index: 15
        rotate-on-boot: true
      enabled: true
      level: ERROR

    category:
      "org.apache.kafka":
        level: INFO
      "io.smallrye.reactive.messaging.kafka":
        level: INFO
      "io.quarkus.config":
        level: INFO
      "io.smallrye.config":
        level: INFO
      "com.csg.airtel.aaa4j":
        level: INFO

  otel:
    traces:
      exporter: none

  # Micrometer Metrics Configuration
  micrometer:
    enabled: true
    binder:
      jvm: true
      system: true
      http-server: true
    export:
      prometheus:
        enabled: true
        path: /q/metrics


#  redis:
#    hosts: redis://localhost:6379
#    max-pool-size: 150               # Right-sized: 300 TPS x ~5ms = ~1.5 concurrent + headroom
#    max-waiting-handlers: 3000
#    max-pool-waiting: 1000
#    pool-recycle-timeout: 240000
#    reconnect-attempts: 3
#    reconnect-interval: 500
#    pool-cleaner-interval: 20000
#    response-timeout: 3000
#    connection-timeout: 2000

  redis:
    client-type: sentinel
    hosts: redis://redis-cluster-headless.csg-aaa-query-cache.svc.cluster.local:26379
    master-name: mymaster
    role: master
    password: S3cure_Redis_Pass_w0rd!
    max-pool-size: 48               # Right-sized for concurrency 12: max ~24 concurrent Redis ops + headroom
    max-waiting-handlers: 500       # Scaled for concurrency 12 - backpressure prevents unbounded waiting
    max-pool-waiting: 250
    pool-recycle-timeout: 240000
    reconnect-attempts: 3
    reconnect-interval: 500
    pool-cleaner-interval: 20000
    timeout: 5000                   # 5s - prevents threads being held on slow Redis (was 120s)

  # CoA HTTP Client Configuration (Vert.x WebClient)
#coa:
#  nas:
#    host: localhost
#    port: 8088

coa:
  nas:
#    host: airtel-aaa-radius-server-service.airtel-aaa.svc.cluster.local
    host: localhost
    port: 8088

webclient:
  max-pool-size: 10                  # Right-sized for 107 TPS/pod HTTP/1.1
  connect-timeout: 5000
  idle-timeout: 60000
  keep-alive: true
  pipelining: true
  pipelining-limit: 10
  http2-max-pool-size: 10            # Right-sized for 107 TPS/pod HTTP/2
  http2-multiplexing-limit: 100
  http2-keep-alive-timeout: 60


# Pool Configuration - Optimized for 1500 TPS (14 pods x 0.6 core / 1GB, ~107 TPS/pod)
# Per pod: 12 connections x 14 pods = 168 total DB connections
pool:
  max-size: 12                      # 14 pods x 12 = 168 total (right-sized for concurrency 12, fits 1GB memory)
  connection-timeout: 5000          # 5 seconds to acquire connection
  idle-timeout: 600000              # 10 minutes idle before closing
  max-lifetime: 1800000             # 30 minutes max connection lifetime
  acquire-increment: 4              # Connections to acquire at once
  prepared-statement-cache-max-size: 256  # Cache prepared statements
  pipelining-enabled: true          # Enable query pipelining
  pipelining-limit: 256             # Max pipelined queries
  event-loop-size: 1                # Match vertx event-loops-pool-size (0.6 core pod)
  tcp-keep-alive: true              # Detect dead connections
  tcp-no-delay: true                # Reduce latency
  pool-cleaner-enabled: true        # Clean idle connections
  pool-cleaner-interval: 60000      # Clean every minute


idle-session:
  enabled: true
  timeout-minutes: 60       # 1 hour default - sessions older than this will be terminated
  scheduler-interval: 30m    # Run every 60 minutes
  batch-size: 1000

  # Process 1000 sessions per batch (increased for scale)
#kafka:
#  bootstrap:
#    servers: kafka-headless.cluster-dc.svc.cluster.local:9092


# Kafka Producer/Consumer Configuration - Optimized for 1500 TPS (14 pods x 0.6 core / 1GB, ~107 TPS/pod)
mp:
  messaging:
    outgoing:
      db-write-events:
        connector: smallrye-kafka
        topic: dctodr-dc
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 5
        retry.backoff.ms: 500
        max.in.flight.requests.per.connection: 5
        compression.type: lz4
        batch.size: 16384              # 16KB - right-sized for ~107 TPS/pod
        linger.ms: 5                   # 5ms batching window - lower TPS needs less accumulation
        buffer.memory: 8388608         # 8MB - fits 1GB pod memory (4 producers × 8MB = 32MB max)
        metadata.max.age.ms: 30000
        request.timeout.ms: 30000
        delivery.timeout.ms: 120000
        reconnect.backoff.ms: 1000
        reconnect.backoff.max.ms: 5000

      accounting-resp-events:
        connector: smallrye-kafka
        topic: accounting-response-dc
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: lz4
        batch.size: 16384              # 16KB - right-sized for ~107 TPS/pod
        linger.ms: 5                   # 5ms batching window
        buffer.memory: 8388608         # 8MB - fits 1GB pod memory

      accounting-cdr-events:
        connector: smallrye-kafka
        topic: cdr-event-dc
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: lz4
        batch.size: 16384              # 16KB - right-sized for ~107 TPS/pod
        linger.ms: 5                   # 5ms batching window
        buffer.memory: 8388608         # 8MB - fits 1GB pod memory

      quota-notification-events:
        connector: smallrye-kafka
        topic: quota-notifications-dc
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: lz4
        batch.size: 16384              # 16KB - right-sized for ~107 TPS/pod
        linger.ms: 5                   # 5ms batching window
        buffer.memory: 8388608         # 8MB - fits 1GB pod memory

    incoming:
      accounting-events:
        connector: smallrye-kafka
        topic: accounting-dc-2
        group.id: accounting-consumer-group
        value.deserializer: io.quarkus.kafka.client.serialization.ObjectMapperDeserializer
        key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
        auto.offset.reset: earliest
        enable.auto.commit: false
        max.poll.records: 500            # Right-sized for 107 TPS/pod
        max.poll.interval.ms: 300000
        session.timeout.ms: 30000
        heartbeat.interval.ms: 10000    # Prevent consumer rebalancing under sustained high load
        fetch.min.bytes: 1              # Reduce latency - don't wait for min bytes
        fetch.max.bytes: 10485760       # 10MB - right-sized for 1GB memory
        max.partition.fetch.bytes: 1048576  # 1MB per partition - fits 1GB memory
        receive.buffer.bytes: 65536     # 64KB socket buffer - right-sized for 1GB memory
        connections.max.idle.ms: 540000 # 9 min - keep broker connections alive under load
        failure-strategy: ignore
        concurrency: 12                 # 12 concurrent slots with backpressure: max ~480 TPS/pod at 25ms avg


