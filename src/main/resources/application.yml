quarkus:
  http:
    root-path: /api/v1
    port: 9905
    io-threads: 8
    limits:
      max-body-size: 10M
      max-connections: 2000
    idle-timeout: 10M
    read-timeout: 30S



  # Database Configuration - HIGH TPS OPTIMIZED for 2500 TPS
  datasource:
    db-kind: oracle
    username: aaa
    password: Aaa!89nky78D
    reactive:
      url: oracle:thin:@localhost:1521/FREEPDB1
#      url: oracle:thin:@localhost:1522/ORCL

  # Vertx Configuration - HIGH TPS OPTIMIZED for 2500 TPS
  vertx:
    event-loops-pool-size: 32
    internal-blocking-pool-size: 100
    warning-exception-time: 10s
    max-event-loop-execute-time: 5s
    max-worker-execute-time: 60s
    prefer-native-transport: true
    caching: false

  thread-pool:
    core-threads: 100
    max-threads: 500
    queue-size: 5000
    growth-resistance: 0
    shutdown-timeout: 30s

  console:
    color: true
  log:
    level: INFO  # Set to DEBUG for better visibility
    console:
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} [%X{requestId:-}] %-5p [%c{3.}] (%t) %s%e%n" # Performance: Enable async console logging
      async-overflow: DISCARD  # Performance: Discard console logs under extreme load
      json:
        enabled: false
    file:
      path: /var/logs/aaa-account-${DEPLOYMENT_MODE:local}-${HOSTNAME:localhost}.log
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} [%X{requestId:-}] %-5p [%c{3.}] (%t) %s%e%n"
      async: true  # Performance: Enable async file logging (critical for high TPS)
      async-overflow: DISCARD  # Performance: Block when queue full to prevent data loss
      json:
        ~: false  # Set to true for production JSON logging
        pretty-print: false  # Compact JSON for better performance
        record-delimiter: "\n"
        excluded-keys:
          - sequence
        additional-field:
          app.name: "aaa-accounting-service"
          app.version: "1.0.0"
          environment: "production"
      rotation:
        max-file-size: 100M
        max-backup-index: 10
        rotate-on-boot: true
      enabled: true

    # Async logging for high-TPS performance (2500+ TPS optimized)
    handler:
      file:
        "ASYNC_FILE":
          level: ALL
          overflow-action: BLOCK  # BLOCK ensures no log loss; DISCARD for max throughput
          queue-length: 4096  # Increased from 2048 for higher TPS (2500 * 2 seconds buffer)
    category:
      "org.apache.kafka":
        level: WARN
      "io.smallrye.reactive.messaging.kafka":
        level: DEBUG
      "io.quarkus.config":
        level: INFO
      "io.smallrye.config":
        level: INFO
      "com.csg.airtel.aaa4j":
        level: INFO  # Debug enabled for application logs

  otel:
    traces:
      exporter: none

  redis:
    hosts: redis://localhost:6379
    max-pool-size: 600
    max-waiting-handlers: 12000
    max-pool-waiting: 3000
    pool-recycle-timeout: 240000
    reconnect-attempts: 3
    reconnect-interval: 500
    pool-cleaner-interval: 20000




# Pool Configuration
pool:
  max-size: 50                      # Max connections
  connection-timeout: 5000          # 5 seconds to acquire connection
  idle-timeout: 600000              # 10 minutes idle before closing
  max-lifetime: 1800000             # 30 minutes max connection lifetime
  acquire-increment: 4              # Connections to acquire at once
  prepared-statement-cache-max-size: 256  # Cache prepared statements
  pipelining-enabled: true          # Enable query pipelining
  pipelining-limit: 256             # Max pipelined queries
  event-loop-size: 32               # Match vertx event-loops-pool-size
  tcp-keep-alive: true              # Detect dead connections
  tcp-no-delay: true                # Reduce latency
  pool-cleaner-enabled: true        # Clean idle connections
  pool-cleaner-interval: 60000      # Clean every minute


idle-session:
  enabled: true
  timeout-minutes: 60       # 1 hour default - sessions older than this will be terminated
  scheduler-interval: 60m    # Run every 60 minutes
  batch-size: 1000          # Process 1000 sessions per batch (increased for scale)

# High-TPS Logging Performance Configuration (2500+ TPS Optimized)
# At 2500 TPS Ã— 7 pods = 17,500 logs/sec baseline
# With sampling enabled, reduces to ~1,750 logs/sec (10x reduction)
logging:
  sampling:
    enabled: false          # Set to 'true' in production for high-TPS environments
    rate: 10                # Log 1 in N requests (10 = 10% sampling, 5 = 20% sampling)
                            # Recommendation: Start with 10 (10%), adjust based on log volume metrics

# Kafka Producer/Consumer Configuration - HIGH TPS OPTIMIZED for 2500 TPS
mp:
  messaging:
    outgoing:
      db-write-events:
        connector: smallrye-kafka
        topic: DC-DR
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 10
        retry.backoff.ms: 500
        max.in.flight.requests.per.connection: 5
        compression.type: lz4
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864
        metadata.max.age.ms: 30000
        request.timeout.ms: 30000
        delivery.timeout.ms: 120000
        reconnect.backoff.ms: 1000
        reconnect.backoff.max.ms: 5000

      #        retries: 3
#        max.in.flight.requests.per.connection: 5
#        compression.type: snappy
#        batch.size: 32768
#        linger.ms: 5
#        buffer.memory: 67108864

      accounting-resp-events:
        connector: smallrye-kafka
        topic: accounting-response
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

      accounting-cdr-events:
        connector: smallrye-kafka
        topic: cdr-event
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

      quota-notification-events:
        connector: smallrye-kafka
        topic: quota-notifications
        value.serializer: io.quarkus.kafka.client.serialization.ObjectMapperSerializer
        key.serializer: org.apache.kafka.common.serialization.StringSerializer
        acks: all
        retries: 3
        max.in.flight.requests.per.connection: 5
        compression.type: snappy
        batch.size: 32768
        linger.ms: 5
        buffer.memory: 67108864

    incoming:
      accounting-events:
        connector: smallrye-kafka
        topic: accounting
        group.id: accounting-consumer-group
        value.deserializer: io.quarkus.kafka.client.serialization.ObjectMapperDeserializer
        key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
        auto.offset.reset: earliest
        enable.auto.commit: false
        max.poll.records: 1000
        max.poll.interval.ms: 300000
        session.timeout.ms: 30000
        fetch.min.bytes: 128
        fetch.max.bytes: 52428800
        max.partition.fetch.bytes: 10485760
        receive.buffer.bytes: 131072
        failure-strategy: ignore

  # ==================== METRICS & MONITORING CONFIGURATION ====================
  # Micrometer metrics configuration for session monitoring and component failures
  micrometer:
    enabled: true
    binder:
      # JVM metrics
      jvm: true
      # System metrics (CPU, memory, disk)
      system: true
      # HTTP server metrics
      http-server: true
      # Vert.x metrics
      vertx: true
    export:
      # Prometheus format endpoint
      prometheus:
        enabled: true
        path: /q/metrics
        default-registry: true

    # ==================== HIGH-TPS DISTRIBUTION STATISTICS ====================
    # Percentile histograms and SLO buckets for latency analysis at 2500+ TPS
    # Memory overhead: ~2KB per timer (HDR Histogram) - negligible at this scale
    # Recording overhead: ~100ns per sample - 0.0002ms at 2500 TPS
    binder-overrides:
      jvm-memory-used:
        enabled: true
      jvm-gc-pause:
        enabled: true

quarkus:
  micrometer:
    binder:
      http-server:
        match-patterns: /api/.*
        ignore-patterns: /q/.*
    export:
      prometheus:
        # Optimize Prometheus export for high cardinality
        descriptions: false  # Reduce export size

    # Distribution statistics configuration for request processing timers
    distribution:
      # RADIUS Request Processing Timers (processing.time)
      # Target SLOs: <100ms (p95), <500ms (p99), <1000ms (p99.9)
      percentiles:
        "[processing.time]": 0.5,0.75,0.95,0.99,0.999
      percentiles-histogram:
        "[processing.time]": true
      slo:
        "[processing.time]": 50ms,100ms,250ms,500ms,1000ms,2000ms
      minimum-expected-value:
        "[processing.time]": 1ms
      maximum-expected-value:
        "[processing.time]": 10000ms

      # Component Operation Timers (Redis, Database, Kafka)
      # Target SLOs: <10ms (Redis), <50ms (DB), <100ms (Kafka)
      percentiles:
        "[component.operation.time]": 0.5,0.75,0.95,0.99,0.999
      percentiles-histogram:
        "[component.operation.time]": true
      slo:
        "[component.operation.time]": 5ms,10ms,25ms,50ms,100ms,500ms
      minimum-expected-value:
        "[component.operation.time]": 1ms
      maximum-expected-value:
        "[component.operation.time]": 5000ms

      # HTTP Server Timers (@Timed annotations)
      percentiles:
        "[http.server.requests]": 0.5,0.75,0.95,0.99
      percentiles-histogram:
        "[http.server.requests]": true
      slo:
        "[http.server.requests]": 50ms,100ms,250ms,500ms,1000ms

mp:

  # SmallRye Health configuration for health checks
  smallrye-health:
    root-path: /q/health
    liveness-path: /q/health/live
    readiness-path: /q/health/ready
    ui:
      enable: true
      root-path: /q/health-ui

# ==================== APPLICATION SPECIFIC MONITORING ====================
# Monitoring service configuration
monitoring:
  metrics:
    export:
      enabled: true
      interval: 30s
      path: /var/log/aaa-account/metrics.json
  health-checks:
    enabled: true
    log-path: /var/log/aaa-account/health-checks.log

